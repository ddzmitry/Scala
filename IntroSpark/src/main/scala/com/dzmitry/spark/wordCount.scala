package com.dzmitry.spark

import org.apache.log4j.{Level, Logger}
import org.apache.spark.sql.SparkSession

object wordCount {

  def main(args: Array[String]): Unit = {

    Logger.getLogger("org").setLevel(Level.ERROR)
    val spark = SparkSession.builder
      .appName("Word Count")  // optional and will be autogenerated if not specified
      .master("local[*]")               // only for demo and testing purposes, use spark-submit instead
      .config("spark.sql.warehouse.dir", "target/spark-warehouse")
      .getOrCreate()

    val lines = spark.sparkContext.textFile("../book.txt")
//    lines.foreach(println)
    val flatMapRdd = lines
      .flatMap(x => x.split("\\W+"))
      .map(x => x.toLowerCase)

//    flatMapRdd.foreach(println)
    val wordToOneRdd = flatMapRdd.map(word => (word,1))
    val couned = wordToOneRdd
      .reduceByKey((x,y)=> (x + y))
//      count,word
      .map((x) => (x._2,x._1))
//      sorting by count
      .sortByKey(false)
//      reverse back to word,count
      .map((x)=>(x._2,x._1))

    couned
      .collect()
      .take(10)
      .foreach(println)


  }
}
